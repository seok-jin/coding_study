## 통계 기반 기법의 문제점
- 데이터를 한번에 처리 (`배치 학습`)
- SVD 경우 수행시간이 __n**3__ => 데이터가 클 경우 현실적으로 불가능
- `추론 기반 기법은` 데이터를 나눠서 처리하는 `미니배치 학습`을 하기 때문에 계산량이 커도 가능함

***

## 추론 기반 기법(신경망)
- 주변 단어(`맥락`)가 주어졌을 때 무슨 단어가 들어가는지를 추론
- 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습함
- 맥락을 입력하면 모델은 출현할 수 있는 각 단어의 출현 확률을 출력한다.
- 말뭉치를 사용해 모델이 올바른 추측을 내놓도록 학습함
- 학습의 결과로 단어의 분산 표현을 얻음
- `통계 기반 기법`처럼 __단어의 의미는 주변 단어에 의해 형성된다는 `분포 가설`에 기초__

### 신경망에서 단어 처리 
- 단어를 `고정 길이 벡터`로 변환
	- one-hot vector 주로 사용
	- 입력층은 뉴런의 수를 `고정`할 수 있음 (feature 수를 어휘의 수만큼)

*** 

## CBOW(Continuous Bag Of Words)
- 맥락(주변 단어)으로 부터 타깃(중앙 단어)를 추측
	- 입력 : 맥락(주변 단어들의 목록)을 one-hot vector로 표현
- `완전연결계층(fully-connected layer)`이 처리
- 입력층이 여러 개 있고 그 입력층들이 가중치를 공유함
- input layer
	- 맥락에서 사용하는 단어 수(윈도우 크기x2) = 입력층의 수(N)
- hidden layer
	- 은닉충의 뉴런은 입력층의 완전연결계층에 의해 변환된 값
	- 입력층이 여러개면 전체를 `평균`함
		- 입력층이 여러개이므로 은닉층도 여러개일 것, 따라서 그 평균을 구해서 은닉층을 하나로 만듬
- output layer
	- 출력층의 뉴런 하나하나는 __각각의 단어에 대응됨__
	- 출력층의 뉴런은 각 단어의 `점수(score)`임 (값이 높을 수록 대응 단어의 출현 확률 높음)
		- 확률로 해석하기 위해 `소프트맥스`함수 적용시키면 됨
		- 이 확률은 맥락(전후 단어)이 주어졌을 떄 그 중앙에 어떤 단어가 출현하는지를 나타낸다.
- 가중치
	- input=>hidden(W_in)
		- 각 `행`이 해당 단어의 `분산 표현`이 담김
		- 학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 이 분산표현들이 갱신될 것임
		- 이렇게 해서 얻은 벡터엔 `단어의 의미`도 잘 녹아들어 있다!
		- __은닉층의 뉴런 수를 입력 층의 뉴런 수보다 적게 해야함__(단어 예측에 필요한 정보를 간결하게 담고, 밀집벡터 표현을 얻기 위해)
	- hidden=>output(W_out)
		- 각 `열`이 각 단어의 `분산 표현`에 해당
		- 단어의 의미가 인코딩된 벡터 (인코딩: 사람이 해석하기 어려운 코드, 디코딩: 인코딩된 거 복원)
		- 단어의 표현이 열방향으로 저장됨
	- __단어의 분산 표현으로는 어떤 가중치를 이용할까?__
		1. 입력층 가중치만 이용 (word2vec에선 주로 입력층만 이용)
		2. 출력층 가중치만 이용
		3. 둘다 이용 (입, 출력 가중치 합치는 등)

### CBOW 학습
- 올바른 예측을 할 수 있도록 가중치 조정
- 가중치(W_in, W_out)에 단어의 출현 패턴을 파악한 벡터가 학습됨
- 말뭉치가 다르면 학습 후 얻게 되는 단어의 분산 표현도 달라짐
- 다중 클래스 분류이므로 소프트맥스 함수를 이용해 점수를 확률로 변환, 그 확률과 레이블로부터 CEE로 구한 loss로 학습함

### CBOW 모델과 확률
- 동시확률: P(A, B), 사후확률: P(A\|B)
- Loss function을 `음의 로그 가능도(negative log likelihood)`로 만들 수 있음
	- 확률에 log 취한 다음 마이너습 붙인거
- CBOW 모델의 학습이 수행하는 일은 이 Loss function의 값을 가능한 작게 만드는 것!

***

## Skip-gram 모델
- CBOW에서 다루는 `맥락`과 `타깃`을 역전시킨 모델
- 중앙의 단어(타깃)를 보고 주변의 여러 단어(맥락)를 추측함
- 입력층 1개, 출력층은 맥락 수만큼 존재
- 각 출력층에서는 개별적으로 손실을 구하고 이 손실들을 다 더한 값을 최종 손실로함
- 맥락의 단어들 사이에 관련성이 없다고(조건부 독립) 가정
	- CEE에서 손실함수값은 로그 성질을 이용해서 맥락별 손실을 구한다음 모두 더한것
- 단어 분산 표현의 정밀도, 성능 면에서 skip-gram 모델의 결과가 CBOW보다 더 좋은 경우가 많음
- 학습 속도는 CBOW가 더 빠름
