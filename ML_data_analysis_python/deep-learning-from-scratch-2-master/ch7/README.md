## 언어 모델 문장 생성 방법
1. 확률이 가장 높은 단어 => 결정적임
2. 확률적으로 선택(확률 분포에 따라) => 매번 결과가 달라질 수 있음


***

## seq2seq 원리
- Encoder-Decoder 모델이라고도 함
- Encoder
	- 마지막 은닉 상태 h에 입력 문장을 번역하는 데 필요한 정보가 들어감
	- 은닉 상태 h는 `고정 길이 벡터`이므로 결국 임의 길이의 문장을 고정 길이 벡터로 변환하는 것!
- Decoder
	- 문장 생성 모델을 이용
	- __LSTM 계층이 벡터 h를 입력받는다는 점만 다름__
	- <eos>를 decoder에 '시작/종료'를 알리는 구분자로 사용

### toy problem(더하기)
- `문자 단위 분할` => 가변 길이 시계열 데이터 => 미니배치 처리시 뭔가 해줘야함
	- `패딩` 사용
		- 패딩 전용 처리 필요함 (decoder에 입력된 정보가 패딩이라면 손실 결과에 반영 X)

### seq2seq 학습 속도 개선
1. 입력 데이터 반전 (Reverse)
	- 기울기 전파가 원활해짐
2. 엿보기 (Peeky)
	- 모든 시각의 Affine, LSTM 계층에 Encoder 출력 h를 전해줌 (h는 중요한 정보이므로!)
	- 가중치 매개변수가 커져서 계산량도 늘어남
	- 정확도는 하이퍼파라미터에 영향을 크게 받음 => 핸디캡 감안해야함

### seq2seq 사용 어플리케이션
- seq2seq는 한 시계열 데이터를 다른 시계열 데이터로 변환하는 것!
- 기계 번역/자동 요약/질의응답/메일 자동 응답 등에서 사용함
- 자연어 외에도 음성이나 영상에서 이용 가능
- 챗봇/알고리즘 학습/이미지 캡셔닝 등에서 활용됨

