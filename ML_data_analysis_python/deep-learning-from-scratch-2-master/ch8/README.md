## seq2seq 문제점
- Encoder 출력 : `고정 길이 벡터`(항상 같은 길이의 벡터로 변환한다.)
	- 문장의 길이에 상관없이 항상 같은 길이의 벡터에 밀어 넣어야 함

### Encoder 개선
- Encoder의 출력 길이를 입력 문장의 길이에 따라 바꿔주자
	- __각 시각(단어)의 은닉 상태 벡터를 모두 이용하자 => 벡터의 집합(행렬)을 출력함__

### Decoder 개선 1
- Encoder 개선에 따라 각 단어의 은닉 상태 벡터를 입력으로 받음
- '입력과 출력의 여러 단어 중 _어떤 단어끼리 서로 관련되어 있는가_'라는 `대응 관계`를 seq2seq에 학습시킬 수는 없을까?
	- `alignment`: 단어의 대응관계를 나타내는 정보 (cat-고양이)
	- `어텐션`은 alignment를 seq2seq에 자동으로 도입함 (이전에는 수작업)
- 우리는 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 목표! (이 구조를 어텐션이라고 한다!)

#### alignment 추출
- Decoder에 입력된 단어와 대응관계인 단어의 벡터를 hs에서 추출한다!
- But, 선택하는 작업은 미분할 수 없음 => `오차역전파법` 사용 불가
- 그럼 선택 작업을 미분 가능한 연산으로 대체 가능할까? => 하나를 선택하는 것이 아니라 __모든 것을 선택하고 각 단어의 중요도를 나타내는 가중치를 별도로 계산하도록 함__
- 각 단어와 가중치를 weighted sum하여 원하는 벡터를 얻는다.
	- 이것을 `맥락 벡터`라고 함

### Decoder 개선 2
- 각 단어의 중요도를 나타내는 가중치를 어떻게 구해야 할까? => 데이터로부터 자동으로 학습하도록
- LSTM 계층의 은닉 상태 벡터 h가 hs의 각 단어와 얼마나 비슷한가를 수치로 나타내는 것
- 벡터의 내적 이용 (두 벡터가 얼마나 같은 방향을 향하고 있는가)
- 벡터의 내적으로 유사도 구하고 소프트맥스 함수 등으로 정규화함

### Decoder 개선 3
- WeightSum 계층과 AttentionWeight 계층을 하나로 구현 (Attention 계층)

### 참고
- WMT : 번역용 데이터셋 중엔 `WMT`가 유명함

***

## 양방향 RNN
- Encoder에서 LSTM을 양방향으로 처리함
- 역방향으로 처리하는 LSTM 계층 추가

## Attention 계층의 사용 방법
- Attention 계층의 출력(맥락 벡터)이 다음 시각의 LSTM 계층에 입력하는 등 사용 방법을 변경할 수 있다!

## seq2seq 심층화와 skip 연결
- seq2seq 더 높은 표현력 => RNN 계층을 깊게 쌓자
- 층을 깊게 할 때 사용 되는 중요한 기술은 `skip 연결`임
	- __skip connection == residual connection == short-cut__
	- 계층을 건너 뛰는 연결
	- skip 연결의 접속부에서는 2개의 출력이 더해짐(원소별 덧셈)
		- 덧셈은 역전파 시 기울기를 그대로 흘려보냄 => skip 연결의 기울기가 아무런 영향을 받지 않고 모든 계층으로 흐름 => 층이 깊어져도 기울기 소실(or 폭발)이 일어나지 않음

***

## 어텐션 응용

### 구글 신경망 기계 번역(Google Neural Machine Translation, GNMT)
- 어텐션 이용
- LSTM 계층의 다층화, 양방향 LSTM, skip 연결, GPU 분산학습

### 트랜스포머
- 시간 방향으로 병렬 처리는 불가능 => RNN을 없애는 연구 or 병렬 계산할 수 있는 RNN 연구
- 논문 <Attention is all you need> - `트랜스포머(transformer)` 모델
	- 이 모델은 _RNN이 아닌 어텐션_ 을 사용해 처리
	- Encoder, Decoder에서 RNN 대신에  `셀프어텐션`을 사용함
	- 피드포워드 신경망
- 셀프어텐션(self-attention)
	- 하나의 시계열 데이터를 대상으로 한 어텐션
	- 하나의 시계열 데이터 내에서 각 원소가 다른 원소들가 어떻게 관련되는지를 봄
- 결국 어텐션을 RNN을 대체하는 모듈로 이용함

### 뉴럴 튜링 머신(NTM)
- `외부 메모리`를 이용함
- 사실 어텐션을 사용한 seq2seq는 메모리조작 같은 작업을 수행함 
	- Encoder가 필요한 정보를 메모리에 쓰고, Decoder는 그 메모리로부터 필요한 정보를 읽어들이는 것
- `NTM`은 RNN 외부에 정보 저장용 메모리 기능을 배치, 어텐션을 이용하여 그 메모리로부터 필요한 정보를 읽거나 쓰는 방법에 대한 연구!
- 컨트롤러
	- 정보를 처리하는 모듈
	- 차례로 들어오는 0, 1 이진 데이터를 처리하여 새로운 데이터를 출력함
	- 컨트롤러의 바깥에는 `메모리`가 존재
- 메모리
	- 메모리 덕분에 컨트롤러는 컴퓨터(혹은 튜링 머신)와 같은 능력을 얻음
		- 메모리에 필요한 정보를 쓰거나 불필요한 정보를 지우는 능력, 필요한 정보를 다시 읽는 능력
- NTM은 외부 메모리를 읽고 쓰면서 시계열 데이터를 처리함
- 메모리 조작을 __미분 가능한 계산__ 으로 구축함 => 메모리 조작 순서도 학습 가능
- 메모리를 읽고 쓰는 계층(데이터의 번지 선택)은 `어텐션`을 이용
	- 미분을 위해 모든 데이터 선택하고, 데이터의 기여도를 나타내는 가중치를 이용
- 2개의 어텐션
	1. 콘텐츠 기반 어텐션
		- 입력으로 주어진 벡터와 비슷한 벡터를 메모리로부터 찾아내는 용도
	2. 위치 기반 어텐션
		- 이전 시각에서 주목한 메모리의 위치를 기준으로 그 전후로 이동(시프트)하는 용도로 사용함
		- 1차원 합성곱 연산으로 구현
		- 메모리 위치를 하나씩 옮겨 가며 읽어나가는 컴퓨터의 움직임을 구현


