## 3장에서 만든 CBOW 모델 문제점
- __다루는 어휘수가 많을 때(거대한 말뭉치) 문제가 발생__
1. 입력층의 원핫벡터와 가중치행렬 W_in의 곱 계산 => `Embedding 계층`으로 해결
2. 은닝층과 가중치 행렬 W_out의 곱 및 Softmax 계층의 계산 => `negative sampling`으로 해결

***

## Embedding 계층
- 사실 입력층과 W_in의 행렬곱은 필요가 없음(`원핫벡터`라 특정 행만 추출하기 때문)
- 그래서 단어 ID에 해당하는 행을 추출하는 계층을 만듬 = Embedding 계층
- Embedding 계층에 단어 임베딩(분산표현)을 저장하는 것

### 단어 인베딩
- NLP에서 단어의 밀집벡터 표현을 `단어 임베딩` 혹은 `분산 표현`이라고 한다.


***

## Negative Sampling
- 은닉층과 W_out 행렬곱, 소프트맥스 계층의 계산 속도 해결
	- 어휘가 많아지면 행렬곱 계산량 증가, 소프트맥스도 마찬가지로 계산량(exp 계산) 증가함
- 어휘가 아무리 많아도 계산량을 낮은 수준에서 일정하게 억제 가능

### 다중 분류(multi-classification)를 이진 분류(binary classification)로 근사하는 것
	- ex) 맥락이 'you'와 'goodbye'일 때 타깃 단어는 'say'입니까? => O/X
- 출력층 뉴런은 1개만 필요함
- 은닉층과 W_out의 내적은 타깃에 해당하는 열벡터만 추출, 그 열벡터와 은닉층 뉴런과의 내적을 계산하면 끝남
- __모든 단어가 아닌 타깃 단어에만 집중해서 점수를 계산한다!__

### 시그모이드 함수와 교차 엔트로피 오차(CEE)
- `이진 분류(binary clasification)` 신경망에서 가장 흔하게 사용하는 조합이다!
	- 보통 `다중 분류` 모델은 `소프트맥스`와 `CEE` 사용
- `시그모이드`함수의 출력은 `확률`로 해석 가능(0~1)
- 역전파는 y-t (출력과 정답의 차이)

### Negative sampling 이란?
- 위의 신경망은 긍정적인 단어(타겟)에 대해서만 학습, __부정적인 단어(타겟 이외의 단어)에 대해선 학습 안함__
- 우리가 하고싶은 것은 긍정적인 예(타겟)에 대해서는 Sigmoid 출력을 1에 가깝게, 부정적인 예(타겟 이외)에 대해서는 Sigmoid 출력을  0으로 만드는 것이다.
- 그러므로 이런 결과를 만들어 주는 `가중치`가 필요하다.
- 그렇다고 모든 부정적인 결과 학습하는건 X (어휘수 증가하기 때문)
- __따라서 부정적인 예를 `샘플링`하자!!__ => `negative sampling`
- __`negative sampling`은 `긍정적인 예`를 타깃으로 한 손실을 구함(레이블:1). 동시에 `부정적 예`를 샘플링해서 그 부정적 예에 대한 손실도 구함(레이블:0)__
- 그리고 각각의 손실을 더한 값이 `최종 손실`이다.

### Negative sampling의 샘플링 기법
- 말뭉치의 `통계 데이터`를 기초로 샘플링하자!
- 즉, 자주 등장하는 단어를 많이 추출하고, 적게 등장하는 단어를 적게 추출하는 것!
- 먼저 말뭉치에서 각 단어의 출현 횟수를 구해 `확률분포`로 나타내고, 그 확률분포대로 단어 샘플링하면 됨
- 단, 계산량 문제 때문에 적은수(5, 10 등)로 샘플링 수를 한정해야함
- word2vec은 확률분포에 `0.75`를 제곱하라고 권고함
	- 출현 확률이 낮은 단어를 _버리지 않기 위해서_

### 참고
- unigram: 하나의 (연속된) 단어
- bigram: 2개의 연속된 단어
- trigram: 3개의 연속된 단어


## word2vec 장점
- word2vec의 단어의 `분산 표현(W_in)`을 사용하여 __king - man + woman = queen__ 같은 `유추 문제(비유 문제)`를 벡터의 덧셈과 뺄셈으로 풀 수 있다.
- `분산 표현`을 이용한 `전이 학습(transfer learning)`
	- 한 분야에서 배운 지식을 다른 분야에도 적용함
- 보통 NLP에서 `큰 말뭉치`로 학습 끝내고, 학습이 끝난 `분산 표현`을 각자의 작업에 이용함
	- 텍스트 분류, 문서 클러스터링, 품사 태그 달기, 감정 분석 등
- __word2vec은 자연어를 벡터로 변환해준다!__
	- `분산 표현`은 단어를 `고정 길이 벡터`로 변환해줌
	- `문장(단어의 흐름)`도 단어의 분산표현을 사용해서 고정길이의 벡터로 변환 가능
		- 가장 간단한 방법은 문장의 각 단어를 분산표현으로 변환하고 그 합을 구하는 것(`bag-of-words`) => 단어의 순서 고려 X
		- 분산 표현과 `RNN` 사용
	- 고정 길이 벡터로의 변환은 아주 중요!! 자연어를 벡터로 변환한다면 일반적인 머신러닝 기법(SVM, 신경망 등)을 적용 가능하다!
	- 이 때, 단어의 `분산 표현 학습`과 `머신러닝 시스템 학습`은 서로 다른 데이터셋을 사용해 개별적으로 수행하는 것이 일반적임

## 분산 표현 평가 방법
- 분산 표현은 특정한 애플리케이션에 이용함 => 여러 시스템으로 구성됨
	- ex) 단어의 분산 표현을 만드는 시스템(word2vec) & 특정 문제에 대해 분류하는 시스템(감정 분석 시스템의 학습)
-  따라서 2단계 학습을 수행한 다음 평가해야함 => 시간 오래 걸림
- 따라서 분산 표현의 우수성을 실제 애플리케이션과는 __분리해서 평가한다__
- 자주 쓰는 평가 척도 : `단어의 유사성`과 `유추 문제`
	- 유사성: 사람이 부여한 점수와 word2vec에 의한 코사인 유사도 점수 비교
- 참고
	1. 모델에 따라 정확도가 다르다. (말뭉치에 따라 적합한 모델 선택)
	2. 일반적으로 말뭉치가 클수록 결과가 좋음 (데이터는 고고익선)
	3. 단어 벡터 차원 수는 적당한 크기가 좋다. (너무 커도 정확도 나빠짐)
	
